{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ba842c",
   "metadata": {
    "papermill": {
     "duration": 0.009173,
     "end_time": "2023-03-22T07:55:10.299174",
     "exception": false,
     "start_time": "2023-03-22T07:55:10.290001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7dd7eb5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.025921,
     "end_time": "2023-03-22T07:55:10.333397",
     "exception": false,
     "start_time": "2023-03-22T07:55:10.307476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import warnings\n",
    "import gc\n",
    "import time\n",
    "import itertools\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, log_loss, roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "import torch\n",
    "from deepctr_torch.models import DeepFM,DCN\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1246d02-b44e-4820-8fb0-23307964b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c440e32f",
   "metadata": {
    "papermill": {
     "duration": 0.008018,
     "end_time": "2023-03-22T07:55:10.349400",
     "exception": false,
     "start_time": "2023-03-22T07:55:10.341382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0237e9",
   "metadata": {
    "papermill": {
     "duration": 138.578101,
     "end_time": "2023-03-22T07:58:08.787943",
     "exception": false,
     "start_time": "2023-03-22T07:55:50.209842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "train = pd.read_csv(\"./data/my_train.csv\", header=0, dtype=str)\n",
    "test_df = pd.read_csv(\"./data/my_test.csv\", header=0, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "870e3e3a",
   "metadata": {
    "papermill": {
     "duration": 0.054681,
     "end_time": "2023-03-22T07:58:26.997010",
     "exception": false,
     "start_time": "2023-03-22T07:58:26.942329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df['data_label'] = 'test'\n",
    "train['data_label'] = 'train'\n",
    "data1 = pd.concat([test_df,train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad73a0b",
   "metadata": {
    "papermill": {
     "duration": 44.5609,
     "end_time": "2023-03-22T07:59:13.443673",
     "exception": false,
     "start_time": "2023-03-22T07:58:28.882773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data1['hour'] = pd.to_datetime(data1['hour'], format = '%y%m%d%H')\n",
    "data1['click'] = pd.to_numeric(data1['click'], errors='coerce')\n",
    "data1['dt'] = data1['hour'].apply(lambda x:str(x)[:10])\n",
    "device_id_sum = data1.groupby(['dt','device_id']).click.sum().reset_index(name='id_sum')\n",
    "device_id_cnt = data1.groupby(['dt','device_id']).click.count().reset_index(name='id_cnt')\n",
    "cnt_sum = pd.merge(device_id_cnt,device_id_sum,on=(['dt','device_id']),how='left')\n",
    "cnt_sum['device_ctr'] = cnt_sum['id_sum']/cnt_sum['id_cnt']\n",
    "cnt_sum = cnt_sum.sort_values(['device_id','dt'])\n",
    "cnt_sum['device_ctr'] = cnt_sum['id_sum']/cnt_sum['id_cnt']\n",
    "cnt_sum = cnt_sum.sort_values(['device_id','dt'])\n",
    "cnt_sum['t_1']=cnt_sum.groupby(['device_id'])['device_ctr'].shift(1)\n",
    "cnt_sum.groupby('device_id')['dt'].count().reset_index().sort_values('dt',ascending=False)\n",
    "data1 = pd.merge(data1,cnt_sum,on=(['dt','device_id']),how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3041c025",
   "metadata": {
    "papermill": {
     "duration": 0.91064,
     "end_time": "2023-03-22T07:59:14.555161",
     "exception": false,
     "start_time": "2023-03-22T07:59:13.644521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data1['hour_of_day'] = data1['hour'].dt.hour\n",
    "data1['weekday'] = (data1['hour'].dt.dayofweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "030c9dc5",
   "metadata": {
    "papermill": {
     "duration": 0.022179,
     "end_time": "2023-03-22T07:59:14.589933",
     "exception": false,
     "start_time": "2023-03-22T07:59:14.567754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_cols = ['site_id',\n",
    " 'site_domain',\n",
    " 'site_category',\n",
    " 'app_id',\n",
    " 'app_domain',\n",
    " 'app_category',\n",
    " 'device_id',\n",
    " 'device_ip',\n",
    " 'device_model','device_type','device_conn_type'\n",
    ",'hour_of_day','weekday','banner_pos']\n",
    "c_cols = ['C14',\n",
    " 'C15',\n",
    " 'C16',\n",
    " 'C17',\n",
    " 'C18',\n",
    " 'C19',\n",
    " 'C20',\n",
    " 'C21','C1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "573b6d0e",
   "metadata": {
    "papermill": {
     "duration": 0.024879,
     "end_time": "2023-03-22T07:59:14.627499",
     "exception": false,
     "start_time": "2023-03-22T07:59:14.602620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sparse特征列\n",
    "sparse_features = obj_cols+['hour_of_day','weekday']+ c_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41471caa",
   "metadata": {
    "papermill": {
     "duration": 2.628958,
     "end_time": "2023-03-22T07:59:17.269712",
     "exception": false,
     "start_time": "2023-03-22T07:59:14.640754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Padding\n",
    "for f in ['device_type','device_conn_type','banner_pos'\n",
    "          ,'C14','C15','C16','C17','C18', 'C19', 'C20', 'C21', 'C1']:\n",
    "    data1[f] = data1[f].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d02e1c48",
   "metadata": {
    "papermill": {
     "duration": 6.259701,
     "end_time": "2023-03-22T07:59:23.541959",
     "exception": false,
     "start_time": "2023-03-22T07:59:17.282258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for f in sparse_features:\n",
    "    data1[f] = data1[f].fillna('-1', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "435b1cbc",
   "metadata": {
    "papermill": {
     "duration": 60.684213,
     "end_time": "2023-03-22T08:00:24.239253",
     "exception": false,
     "start_time": "2023-03-22T07:59:23.555040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 对sparse onehot\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data1[feat] = lbe.fit_transform(data1[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67889183",
   "metadata": {
    "papermill": {
     "duration": 1.985727,
     "end_time": "2023-03-22T08:00:26.293421",
     "exception": false,
     "start_time": "2023-03-22T08:00:24.307694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Continuous Features\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "dense_features = ['id_cnt',\n",
    " 'id_sum',\n",
    " 'device_ctr',\n",
    " 't_1']\n",
    "for f in dense_features:\n",
    "    data1[f] = data1[f].fillna(0, )\n",
    "\n",
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "data1[dense_features] = mms.fit_transform(data1[dense_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a091ade4",
   "metadata": {
    "papermill": {
     "duration": 0.751312,
     "end_time": "2023-03-22T08:00:27.058908",
     "exception": false,
     "start_time": "2023-03-22T08:00:26.307596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2.count #unique features for each sparse field,and record dense feature field name\n",
    "\n",
    "fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=data1[feat].nunique(),embedding_dim=4 )\n",
    "                       for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)\n",
    "                      for feat in dense_features]\n",
    "# fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=data1[feat].nunique(),embedding_dim=4 )\n",
    "#                        for i,feat in enumerate(sparse_features)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "936d8427",
   "metadata": {
    "papermill": {
     "duration": 0.025897,
     "end_time": "2023-03-22T08:00:27.099369",
     "exception": false,
     "start_time": "2023-03-22T08:00:27.073472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dnn_feature_columns = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc926dc6-a2b9-4ef9-a2a3-2f4315eccb99",
   "metadata": {},
   "source": [
    "# Create Dataset 1 and Test Set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a093df44",
   "metadata": {
    "papermill": {
     "duration": 1.56763,
     "end_time": "2023-03-22T08:00:28.886951",
     "exception": false,
     "start_time": "2023-03-22T08:00:27.319321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = data1[data1['data_label']=='train']\n",
    "Dataset_1 = train[:27000]\n",
    "test_1 = data1[data1['data_label']=='test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3029e032",
   "metadata": {
    "papermill": {
     "duration": 0.029333,
     "end_time": "2023-03-22T08:00:29.279344",
     "exception": false,
     "start_time": "2023-03-22T08:00:29.250011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_1_model_input = {name:Dataset_1[name] for name in feature_names}\n",
    "\n",
    "test_1_model_input = {name:test_1[name] for name in feature_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94d0c9-3efe-41b6-a59e-6eeef46c8d4e",
   "metadata": {},
   "source": [
    "# Train Model 1 and 8 on Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33f071b9",
   "metadata": {
    "papermill": {
     "duration": 0.030655,
     "end_time": "2023-03-22T08:00:29.324801",
     "exception": false,
     "start_time": "2023-03-22T08:00:29.294146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deepctr_torch.models import DeepFM,AFM,xDeepFM,AutoInt,DCN,DIFM\n",
    "device = 'cpu'\n",
    "use_cuda = True\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    print('cuda ready...')\n",
    "    device = 'cuda:0'\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75add29e-bf24-4768-af9c-a2a79fd868da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Train on 24300 samples, validate on 2700 samples, 190 steps per epoch\n",
      "Epoch 1/1\n",
      "4s - loss:  0.4312 - binary_crossentropy:  0.4312 - val_binary_crossentropy:  0.4038\n"
     ]
    }
   ],
   "source": [
    "model_1 = DCN(linear_feature_columns,dnn_feature_columns,task='binary',device=device)\n",
    "model_1.compile(\"adam\", \"binary_crossentropy\",metrics=['binary_crossentropy'], )\n",
    "history = model_1.fit(train_1_model_input,Dataset_1['click'].values,batch_size=128,epochs=1,verbose=2,validation_split=0.1)\n",
    "\n",
    "test_1['pred_1']= model_1.predict(test_1_model_input, batch_size=128)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ce50a43-45e8-42d5-ba69-970e0f67743c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Train on 24300 samples, validate on 2700 samples, 190 steps per epoch\n",
      "Epoch 1/1\n",
      "4s - loss:  0.4268 - binary_crossentropy:  0.4267 - val_binary_crossentropy:  0.3968\n"
     ]
    }
   ],
   "source": [
    "model_best = DeepFM(linear_feature_columns,dnn_feature_columns,task='binary',device=device)\n",
    "model_best.compile(\"adam\", \"binary_crossentropy\",metrics=['binary_crossentropy'], )\n",
    "history = model_best.fit(train_1_model_input,Dataset_1['click'].values,batch_size=128,epochs=1,verbose=2,validation_split=0.1)\n",
    "test_1['pred_best']= model_best.predict(test_1_model_input, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bfbc96-6482-46cf-b4f5-362c90648a38",
   "metadata": {},
   "source": [
    "# Create Dataset 2 and Save the offline Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dba1d831-1113-435b-80bf-6bb68dd0f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2 = train[30000:360000]\n",
    "train_2_model_input = {name:train_2[name] for name in feature_names}\n",
    "train_2['pred']= model_1.predict(train_2_model_input, batch_size=128)\n",
    "Dataset_2 = train_2.groupby(train_2.index // 10).apply(lambda x: x.loc[x[\"pred\"].idxmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79215a9b-894e-498d-8214-33e1a9ed9b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_set_2 = Dataset_2[:3000]\n",
    "Test_set_2_model_input = {name:Test_set_2[name] for name in feature_names}\n",
    "new_train=Dataset_2[3000:]\n",
    "new_train_model_input = {name:new_train[name] for name in feature_names}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81ab1b8a-bece-4587-98e9-09fce85598dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_2['click'].to_csv(\"data/offline_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b1def-aff7-4524-9420-f2184fe9d99b",
   "metadata": {},
   "source": [
    "# Train Model 2-7 on Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5abf970-d3c6-4f8b-a12f-3ed820d870bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Train on 27000 samples, validate on 3000 samples, 422 steps per epoch\n",
      "Epoch 1/1\n",
      "11s - loss:  0.5953 - binary_crossentropy:  0.5953 - val_binary_crossentropy:  0.5768\n"
     ]
    }
   ],
   "source": [
    "model2 = DIFM(linear_feature_columns,dnn_feature_columns,task='binary',device=device)\n",
    "model2.compile(\"adam\", \"binary_crossentropy\",metrics=['binary_crossentropy'], )\n",
    "history = model2.fit(new_train_model_input,new_train['click'].values,batch_size=64,epochs=1,verbose=2,validation_split=0.1)\n",
    "test_1['pred_2']= model2.predict(test_1_model_input, batch_size=128)\n",
    "\n",
    "model3 = DeepFM(linear_feature_columns,dnn_feature_columns,task='binary',device=device)\n",
    "model3.compile(\"adam\", \"binary_crossentropy\",metrics=['binary_crossentropy'], )\n",
    "history = model3.fit(new_train_model_input,new_train['click'].values,batch_size=64,epochs=1,verbose=2,validation_split=0.1)\n",
    "test_1['pred_3']= model3.predict(test_1_model_input, batch_size=128)\n",
    "\n",
    "model4 = DCN(linear_feature_columns,dnn_feature_columns,task='binary',device=device)\n",
    "model4.compile(\"adam\", \"binary_crossentropy\",metrics=['binary_crossentropy'], )\n",
    "history = model4.fit(new_train_model_input,new_train['click'].values,batch_size=64,epochs=1,verbose=2,validation_split=0.1)\n",
    "test_1['pred_4']= model4.predict(test_1_model_input, batch_size=128)\n",
    "\n",
    "model5 = DIFM(linear_feature_columns,dnn_feature_columns,task='binary',device=device, dnn_hidden_units=(256, 128, 64),)\n",
    "model5.compile(\"adam\", \"binary_crossentropy\",metrics=['binary_crossentropy'], )\n",
    "history = model5.fit(new_train_model_input,new_train['click'].values,batch_size=64,epochs=1,verbose=2,validation_split=0.1)\n",
    "test_1['pred_5']= model5.predict(test_1_model_input, batch_size=128)\n",
    "\n",
    "model6 = DeepFM(linear_feature_columns,dnn_feature_columns,task='binary',device=device, dnn_hidden_units=(128, 128, 64))\n",
    "model6.compile(\"adam\", \"binary_crossentropy\",metrics=['binary_crossentropy'], )\n",
    "history = model6.fit(new_train_model_input,new_train['click'].values,batch_size=64,epochs=1,verbose=2,validation_split=0.1)\n",
    "test_1['pred_6']= model6.predict(test_1_model_input, batch_size=128)\n",
    "\n",
    "model7 = DCN(linear_feature_columns,dnn_feature_columns,task='binary',device=device, dnn_hidden_units=(256, 128, 64),)\n",
    "model7.compile(\"adam\", \"binary_crossentropy\",metrics=['binary_crossentropy'], )\n",
    "history = model7.fit(new_train_model_input,new_train['click'].values,batch_size=64,epochs=1,verbose=2,validation_split=0.1)\n",
    "test_1['pred_7']= model7.predict(test_1_model_input, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f2988c-b2c0-4d45-a0eb-ed10b7846630",
   "metadata": {},
   "source": [
    "# Evaluate all models on the Two test sets and generate reward table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b62d8-b9de-4b99-981c-a94c42f92e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define Binary Cross-Entropy loss function\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ff31b-bbda-496c-a69e-85e201c10a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1[\"pred_1\"]= model_1.predict(test_1_model_input, batch_size=128)\n",
    "test_1[\"pred_2\"]= model2.predict(test_1_model_input, batch_size=128)\n",
    "test_1[\"pred_3\"]= model3.predict(test_1_model_input, batch_size=128)\n",
    "test_1[\"pred_4\"]= model4.predict(test_1_model_input, batch_size=128)\n",
    "test_1[\"pred_5\"]= model5.predict(test_1_model_input, batch_size=128)\n",
    "test_1[\"pred_6\"]= model6.predict(test_1_model_input, batch_size=128)\n",
    "test_1[\"pred_7\"]= model7.predict(test_1_model_input, batch_size=128)\n",
    "\n",
    "test_1[\"pred_best\"]= model_best.predict(test_1_model_input, batch_size=128)\n",
    "\n",
    "for num_model in [\"pred_1\",\"pred_2\",\"pred_3\",\"pred_4\",\"pred_5\",\"pred_6\",\"pred_7\",\"pred_best\"]:\n",
    "    print(num_model, \", binary cross entropy loss: \", criterion(torch.tensor(test_1[num_model].values, dtype=torch.float32)\n",
    "                                                                ,torch.tensor(test_1[\"click\"].values, dtype=torch.float32)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce800d9-fe09-4253-a9ce-a147ad8581b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_set_2[\"pred_1\"]= model.predict(Test_set_2_model_input, batch_size=128)\n",
    "Test_set_2[\"pred_2\"]= model2.predict(Test_set_2_model_input, batch_size=128)\n",
    "Test_set_2[\"pred_3\"]= model3.predict(Test_set_2_model_input, batch_size=128)\n",
    "Test_set_2[\"pred_4\"]= model4.predict(Test_set_2_model_input, batch_size=128)\n",
    "Test_set_2[\"pred_5\"]= model5.predict(Test_set_2_model_input, batch_size=128)\n",
    "Test_set_2[\"pred_6\"]= model6.predict(Test_set_2_model_input, batch_size=128)\n",
    "Test_set_2[\"pred_7\"]= model7.predict(Test_set_2_model_input, batch_size=128)\n",
    "\n",
    "Test_set_2[\"pred_best\"]= model_best.predict(Test_set_2_model_input, batch_size=128)\n",
    "\n",
    "for num_model in [\"pred_1\",\"pred_2\",\"pred_3\",\"pred_4\",\"pred_5\",\"pred_6\",\"pred_7\",\"pred_best\"]:\n",
    "    print(num_model, \", binary cross entropy loss: \", criterion(torch.tensor(Test_set_2[num_model].values, dtype=torch.float32)\n",
    "                                                                ,torch.tensor(Test_set_2[\"click\"].values, dtype=torch.float32)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c6395b-3222-4b7e-b4b9-3190ea73c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward1 = test_1.groupby(test_1.index // 10).apply(lambda x: x.loc[x[\"pred_1\"].idxmax()])[[\"click\"]]\n",
    "for num_model in [\"pred_2\",\"pred_3\",\"pred_4\",\"pred_5\",\"pred_6\",\"pred_7\",\"pred_best\"]:\n",
    "    rw =test_1.groupby(test_1.index // 10).apply(lambda x: x.loc[x[num_model].idxmax()])[[\"click\"]]\n",
    "    reward1[num_model] = rw[\"click\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb0fce-612a-4c09-a563-9af88074c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward1.rename(columns={'click': 'pred_1'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac7f32e-e651-47a3-906c-12384c9c32a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward1.to_csv(\"data/reward_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae7a1a9-4e81-4c3e-a777-afa950f09400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f0b4fb-c8dc-42d0-bdcd-ad8d537653e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 546.877315,
   "end_time": "2023-03-22T08:04:07.325930",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-22T07:55:00.448615",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
